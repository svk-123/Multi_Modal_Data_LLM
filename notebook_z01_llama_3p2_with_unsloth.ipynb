{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "140effdb-08aa-4420-81cc-4c8186afe67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45abc09e-a76c-4448-af71-001048be0571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BitsAndBytesConfig\n",
    "# quant_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,  # Use True for 4-bit, False for 8-bit\n",
    "#     bnb_4bit_compute_dtype=\"float16\",  # Recommended dtype\n",
    "#     bnb_4bit_use_double_quant=True,  # Enables double quantization\n",
    "#     bnb_4bit_quant_type=\"nf4\"  # Normalized float4 (better for LLMs)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f57a81bc-a22a-44d8-a837-0ac0a213d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use a pipeline as a high-level helper\n",
    "# from transformers import pipeline\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "# ]\n",
    "# pipe = pipeline(\"text-generation\", model=\"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\")\n",
    "# pipe(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e907e2-d499-44ab-b22b-cc3cb1e48a32",
   "metadata": {},
   "source": [
    "### Llama 3.2 11B Vision model in 4bit config generates around 1024 tokens per min with RTX4070 (16GB)\n",
    "### how to speed up the inference with limited GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b3b10d1-e265-41e0-8e7e-cc860d85ac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load model directly\n",
    "# from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "\n",
    "# processor = AutoProcessor.from_pretrained(\"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\")\n",
    "# model = AutoModelForImageTextToText.from_pretrained(\"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\")\n",
    "\n",
    "\n",
    "# # Define the text input (prompt)\n",
    "# instruction = \"Write a creative story about an astronaut exploring a new planet.\"\n",
    "\n",
    "# # Process the text input (no image required)\n",
    "# inputs = processor(text=instruction, return_tensors=\"pt\")\n",
    "\n",
    "# # Ensure that input_ids are on the same device as the model\n",
    "# device = model.device\n",
    "# inputs = inputs.to(device)  # Move inputs to the correct device\n",
    "\n",
    "# start_time = time.time()\n",
    "# # Perform inference using the model\n",
    "# outputs = model.generate(**inputs,max_new_tokens=1024)\n",
    "# end_time = time.time()\n",
    "\n",
    "# # Calculate the time taken\n",
    "# elapsed_time = end_time - start_time\n",
    "# print(f\"Time taken to generate 1024 tokens: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# # Decode and print the output\n",
    "# decoded_output = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "# print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a58716c3-ff33-4675-8b1d-8fd6d0dbb4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.2.12: Fast Mllama vision patching. Transformers: 4.46.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4070 Ti SUPER. Max memory: 15.693 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f7292de71948198a4846a1408ef63c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
    "import torch\n",
    "\n",
    "modelV, tokenizerV = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/Llama-3.2-11B-Vision-Instruct\",\n",
    "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88f5212-96bc-4f0f-84b1-dc339ce41d35",
   "metadata": {},
   "source": [
    "### Llama 3.2 11B Vision model (unsloth) in 4bit config generates around 1500 tokens per min with RTX4070 (16GB)\n",
    "### ### Image + text to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dab09d8-e245-474e-b7d9-4d4a815bb1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image appears to show the rotational forces exerted on an object when the force is applied with the thumb of a hand at point A, as well as an angle Î¸, a perpendicular line, the point where the force is applied at A, and point Z on the rotational axis. It's difficult to discern the entire image.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "FastVisionModel.for_inference(modelV) # Enable for inference!\n",
    "\n",
    "image_path = \"ex1.jpg\"  # Change this to your image path\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "instruction = \"Describe accurately what you see in this image.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": instruction}\n",
    "    ]}\n",
    "]\n",
    "input_text = tokenizerV.apply_chat_template(messages, add_generation_prompt = True)\n",
    "inputs = tokenizerV(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens = False,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizerV, skip_prompt = True)\n",
    "\n",
    "_ = modelV.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0310ec-903d-4ec3-9cba-644364be3347",
   "metadata": {},
   "source": [
    "### Llama 3.2 3B text only model (unsloth) in 4bit config generates around ?? tokens per min with RTX4070 (16GB)\n",
    "### ### Text to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6a33e07-fb24-4074-9f46-1734b2d08207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.46.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4070 Ti SUPER. Max memory: 15.693 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d444947697754068a45d0a04caf638be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.03G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f2fdeda785f4b0e82fa9a46a13a8717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f996d4bb3df842b89e9c5f452d0aad2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b014715a80334f4f81f22a016c7378b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f132faec1d24f2b8daa2b86babe593d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models\n",
    "fourbit_models = [\n",
    "    \"unsloth/Llama-3.2-1B-bnb-4bit\",           # NEW! Llama 3.2 models\n",
    "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
    "] \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b0cfa41-8f02-4873-9968-c789d2b9ba92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|im_start|>user\\nAnswer the best option only : Inertia of a body depends on                                a) weight of the object                                b) acceleration due to gravity of the planet                                c) mass of the object                                d) Both a & b<|im_end|>\\n<|im_start|>assistant\\nThe best answer is (b) acceleration due to gravity of the planet.<|im_end|>']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Chatml style format\n",
    "### Uses OpenAIâ€™s chatml format, which structures messages in a way that aligns with popular models like Zephyr, Mistral, LLaMA, etc.\n",
    "### Converts roles (user, assistant) into a standardized format (human, gpt) to match the expected input for the model.\n",
    "### Ensures that the model properly recognizes the end-of-sentence token (</s>), preventing endless text generation.\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"chatml\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    "    map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"from\": \"human\", \"value\": \"Answer the best option only:\\ Inertia of a body depends on\\\n",
    "                                a) weight of the object\\\n",
    "                                b) acceleration due to gravity of the planet\\\n",
    "                                c) mass of the object\\\n",
    "                                d) Both a & b\"\n",
    "    },\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 1024, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e7e617b-1a0b-486b-82f8-e46799f26e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "write a story about an astronaut<|im_end|>\n",
      "<|im_start|>assistant\n",
      "As the spaceship soared through the cosmos, astronaut Jack Harris gazed out at the endless expanse of stars and planets. He had spent years training for this moment, and now he was finally on his way to Mars.\n",
      "\n",
      "Jack's heart swelled with pride as he thought about the incredible journey that had brought him to this point. From his childhood dreams of becoming an astronaut to the grueling training sessions and endless research on the cutting-edge spacecraft, Jack had worked tirelessly to reach his goal.\n",
      "\n",
      "As he floated in the ship's cramped cabin, Jack felt a sense of peace wash over him. He was surrounded by the familiar sights and sounds of his spacecraft, the silence a comforting respite from the constant hum of machinery.\n",
      "\n",
      "The ship's computer, an efficient and reliable companion, began to monitor Jack's vital signs and perform routine checks on the ship's systems. Jack's eyes flicked to the viewscreen, where the stunning Martian landscape unfolded before him.\n",
      "\n",
      "\"Captain Harris, we're entering Mars' orbit,\" announced the ship's AI, its smooth voice a reassuring presence in the cramped cabin.\n",
      "\n",
      "Jack's gaze snapped to the viewscreen, where the Martian landscape unfolded in breathtaking detail. The rust-red terrain stretched out before him, dotted with towering volcanoes and vast, barren plains.\n",
      "\n",
      "\"Beautiful,\" Jack breathed, a sense of wonder washing over him. \"This is it. This is where I'm meant to be.\"\n",
      "\n",
      "As the ship prepared for its historic landing, Jack couldn't help but think about the incredible journey that had brought him to this moment. He had faced countless challenges and overcome seemingly insurmountable obstacles, and now he was on the cusp of a new chapter in his life.\n",
      "\n",
      "The ship's engines died, and Jack felt a strange sense of calm wash over him. He was no longer just a astronaut; he was a pioneer, a trailblazer, and a hero.\n",
      "\n",
      "As the ship's systems hummed back to life, Jack smiled, knowing that this was just the beginning of an incredible journey. He was ready to take on the challenges of the cosmos, to explore the unknown, and to push the boundaries of human possibility.\n",
      "\n",
      "And so, Jack Harris stepped out of the ship and onto the Martian surface, his heart full of wonder and his spirit full of adventure. The universe was full of mysteries and wonders, and he was ready to face them head-on.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "### text streamer format\n",
    "from unsloth import FastLanguageModel\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"from\": \"human\", \"value\": \"write a story about an astronaut\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1024, use_cache = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e190a06f-787b-4d08-9f0b-80f249099fae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
