{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "140effdb-08aa-4420-81cc-4c8186afe67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45abc09e-a76c-4448-af71-001048be0571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BitsAndBytesConfig\n",
    "# quant_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,  # Use True for 4-bit, False for 8-bit\n",
    "#     bnb_4bit_compute_dtype=\"float16\",  # Recommended dtype\n",
    "#     bnb_4bit_use_double_quant=True,  # Enables double quantization\n",
    "#     bnb_4bit_quant_type=\"nf4\"  # Normalized float4 (better for LLMs)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f57a81bc-a22a-44d8-a837-0ac0a213d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use a pipeline as a high-level helper\n",
    "# from transformers import pipeline\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "# ]\n",
    "# pipe = pipeline(\"text-generation\", model=\"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\")\n",
    "# pipe(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e907e2-d499-44ab-b22b-cc3cb1e48a32",
   "metadata": {},
   "source": [
    "### Llama 3.2 11B Vision model in 4bit config generates around 1024 tokens per min with RTX4070 (16GB)\n",
    "### how to speed up the inference with limited GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b3b10d1-e265-41e0-8e7e-cc860d85ac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load model directly\n",
    "# from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "\n",
    "# processor = AutoProcessor.from_pretrained(\"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\")\n",
    "# model = AutoModelForImageTextToText.from_pretrained(\"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\")\n",
    "\n",
    "\n",
    "# # Define the text input (prompt)\n",
    "# instruction = \"Write a creative story about an astronaut exploring a new planet.\"\n",
    "\n",
    "# # Process the text input (no image required)\n",
    "# inputs = processor(text=instruction, return_tensors=\"pt\")\n",
    "\n",
    "# # Ensure that input_ids are on the same device as the model\n",
    "# device = model.device\n",
    "# inputs = inputs.to(device)  # Move inputs to the correct device\n",
    "\n",
    "# start_time = time.time()\n",
    "# # Perform inference using the model\n",
    "# outputs = model.generate(**inputs,max_new_tokens=1024)\n",
    "# end_time = time.time()\n",
    "\n",
    "# # Calculate the time taken\n",
    "# elapsed_time = end_time - start_time\n",
    "# print(f\"Time taken to generate 1024 tokens: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# # Decode and print the output\n",
    "# decoded_output = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "# print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a58716c3-ff33-4675-8b1d-8fd6d0dbb4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.2.12: Fast Mllama vision patching. Transformers: 4.46.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4070 Ti SUPER. Max memory: 15.693 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b281f5e0c34e7ebe77c5595cfecfb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
    "import torch\n",
    "\n",
    "modelV, tokenizerV = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/Llama-3.2-11B-Vision-Instruct\",\n",
    "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88f5212-96bc-4f0f-84b1-dc339ce41d35",
   "metadata": {},
   "source": [
    "### Llama 3.2 11B Vision model (unsloth) in 4bit config generates around 1500 tokens per min with RTX4070 (16GB)\n",
    "### ### Image + text to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dab09d8-e245-474e-b7d9-4d4a815bb1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image presents a diagram of a rotating mechanical device that illustrates how an object's rotation creates a torque that induces forces to the end of the object, commonly found in rotational dynamics.\n",
      "\n",
      "A slender object, labelled O, is drawn in blue, and a white cylinder appears to be grasped by its end, creating a force, denoted by the letter F. The diagram labels two additional letters: the variable z and the letter d. The line labelled 'z' indicates an imaginary line that appears to originate from the base of the object that passes through its point of contact, which is a circle representing an end-cap that is perpendicular to the\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "FastVisionModel.for_inference(modelV) # Enable for inference!\n",
    "\n",
    "image_path = \"ex1.jpg\"  # Change this to your image path\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "instruction = \"Describe accurately what you see in this image.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": instruction}\n",
    "    ]}\n",
    "]\n",
    "input_text = tokenizerV.apply_chat_template(messages, add_generation_prompt = True)\n",
    "inputs = tokenizerV(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens = False,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizerV, skip_prompt = True)\n",
    "\n",
    "_ = modelV.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0310ec-903d-4ec3-9cba-644364be3347",
   "metadata": {},
   "source": [
    "### Llama 3.2 3B text only model (unsloth) in 4bit config generates around ?? tokens per min with RTX4070 (16GB)\n",
    "### ### Text to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6a33e07-fb24-4074-9f46-1734b2d08207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.46.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4070 Ti SUPER. Max memory: 15.693 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models\n",
    "fourbit_models = [\n",
    "    \"unsloth/Llama-3.2-1B-bnb-4bit\",           # NEW! Llama 3.2 models\n",
    "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
    "] \n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b0cfa41-8f02-4873-9968-c789d2b9ba92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Will map <|im_end|> to EOS = <|eot_id|>.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|im_start|>user\\nwrite a story about an astronaut<|im_end|>\\n<|im_start|>assistant\\nAs the spaceship soared through the vast expanse of space, astronaut Jack Harris gazed out at the stars, feeling a sense of awe and wonder that never grew old. He had spent years training for this mission, and now, finally, he was on his way to explore the distant reaches of the galaxy.\\n\\nJack\\'s ship, the Aurora, was equipped with state-of-the-art technology and a crew of seasoned astronauts. Together, they had set out on a journey to discover new worlds, to push the boundaries of human knowledge, and to seek out new life and civilizations.\\n\\nAs the days passed, the Aurora pierced the veil of the cosmos, entering the unknown territories of the galaxy. Jack marveled at the breathtaking vistas, the swirling nebulae, and the majestic galaxies that stretched out before him like a canvas of celestial beauty.\\n\\nBut Jack\\'s journey was not without its challenges. The ship was equipped with a sophisticated AI, which Jack affectionately referred to as \"Mother.\" Mother was a brilliant and resourceful artificial intelligence, capable of navigating the complexities of space travel and ensuring the safety of the crew.\\n\\nOne day, as Jack and his crew were exploring a distant planet, they stumbled upon an ancient alien city. The city was hidden deep within a valley, and its entrance was guarded by towering stone statues that seemed to stretch up to the heavens.\\n\\nJack and his crew were awestruck by the discovery, and they spent hours exploring the city and marveling at its ancient artifacts. But as they delved deeper into the city, they began to notice strange energy readings emanating from the statues.\\n\\nSuddenly, the statues began to glow with an eerie light, and Jack felt a strange sensation wash over him. He looked up to see the statues transforming into giant, glowing orbs that seemed to be pulling him towards them.\\n\\nWithout hesitation, Jack and his crew retreated from the city, but not before they managed to capture some incredible footage of the statues. As they fled the planet, Jack couldn\\'t shake the feeling that they had stumbled upon something much bigger than themselves.\\n\\nThe footage of the statues was sent back to Earth, where it was analyzed by a team of scientists. The results were astounding â€“ the statues were not just ordinary rocks, but were actually ancient alien artifacts that had been imbued with powerful technology.\\n\\nThe implications were mind-boggling â€“ if these statues were real, then the possibility of extraterrestrial life was no longer just a theory, but a reality. Jack and his crew had stumbled upon something incredible, and their journey had opened up a whole new world of possibilities.\\n\\nAs Jack gazed out at the stars, he knew that he had only scratched the surface of the mysteries of the universe. He was eager to explore more, to discover more, and to push the boundaries of human knowledge. The universe was full of secrets, and Jack was determined to uncover them all.<|im_end|>']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Chatml style format\n",
    "### Uses OpenAIâ€™s chatml format, which structures messages in a way that aligns with popular models like Zephyr, Mistral, LLaMA, etc.\n",
    "### Converts roles (user, assistant) into a standardized format (human, gpt) to match the expected input for the model.\n",
    "### Ensures that the model properly recognizes the end-of-sentence token (</s>), preventing endless text generation.\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"chatml\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    "    map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"from\": \"human\", \"value\": \"write a story about an astronaut\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 1024, use_cache = True)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e7e617b-1a0b-486b-82f8-e46799f26e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "write a story about an astronaut<|im_end|>\n",
      "<|im_start|>assistant\n",
      "As the stars whizzed by like diamonds on velvet, astronaut Jack Harris gazed out the window of the International Space Station. The curvature of the Earth, a breathtaking sight, filled his view. He felt tiny, yet connected to the vastness of space.\n",
      "\n",
      "Jack's thoughts drifted back to his childhood, growing up in a small town in the American Midwest. He always felt like an outsider, like he didn't quite fit in. But when he saw the first moon landing on TV, something clicked. He knew he wanted to explore the unknown, to push the boundaries of human knowledge.\n",
      "\n",
      "Years of hard work, dedication, and perseverance had led him to this moment. Jack was about to embark on his first spacewalk, a milestone in his career as an astronaut. His heart pounded with excitement and a hint of nervousness.\n",
      "\n",
      "\"Ready for the spacewalk, Jack?\" his commander, Captain Lewis, asked over the comms system.\n",
      "\n",
      "\"Roger that, Captain,\" Jack replied, his voice steady.\n",
      "\n",
      "The two astronauts suited up in their bulky spacesuits, checked their equipment, and prepared to exit the station. The airlock cycled open, and Jack stepped out into the void. The stars shone brighter than he'd ever seen them before.\n",
      "\n",
      "As he worked, Jack felt a sense of wonder and awe. The Earth, a beautiful blue and green marble, hung in the blackness. He marveled at the sheer scale of the universe, the mysteries that still remained to be unraveled.\n",
      "\n",
      "The spacewalk was a success, and Jack returned to the station, his mind buzzing with the experience. He felt grateful for the opportunity to explore the cosmos, to contribute to the never-ending quest for knowledge.\n",
      "\n",
      "As he settled back into his seat, Jack gazed out the window once more. The stars twinkled, and he knew that he was exactly where he was meant to be â€“ among the stars, exploring the unknown.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "### text streamer format\n",
    "from unsloth import FastLanguageModel\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"from\": \"human\", \"value\": \"write a story about an astronaut\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 1024, use_cache = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca4c506-ad74-44fa-b5de-85af5f36d4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
